


# ğŸš— Vehicle Data ML Project

An end-to-end **ML pipeline project** integrating **MongoDB, AWS, Docker, GitHub Actions, and CI/CD**.
This repo demonstrates **data ingestion, validation, transformation, training, evaluation, and deployment** with a scalable architecture.

---

## ğŸ“‚ Project Structure

```
project/
â”‚-- .github/workflows/        # GitHub Actions (CI/CD)
â”‚-- notebook/                 # Jupyter notebooks (EDA, MongoDB demo, Feature Engg.)
â”‚-- src/                      # Source code
â”‚   â”œâ”€â”€ components/           # Data ingestion, validation, transformation, training
â”‚   â”œâ”€â”€ configuration/        # DB & AWS connections
â”‚   â”œâ”€â”€ data_access/          # MongoDB data fetch logic
â”‚   â”œâ”€â”€ entity/               # Configs, artifacts, estimators
â”‚   â””â”€â”€ utils/                # Utility scripts
â”‚-- requirements.txt
â”‚-- setup.py
â”‚-- pyproject.toml
â”‚-- app.py                    # Flask app for prediction pipeline
```

---

## âš¡ Getting Started

### 1ï¸âƒ£ Project Initialization

```bash
# Create project template
python template.py
```

### 2ï¸âƒ£ Local Package Setup

* Add code to **setup.py** and **pyproject.toml** to import local packages
  (see details in `crashcourse.txt`)

---

## ğŸ Virtual Environment & Requirements

```bash
conda create -n vehicle python=3.10 -y
conda activate vehicle

# Add dependencies to requirements.txt, then:
pip install -r requirements.txt

# Verify installations
pip list
```

---

## ğŸƒ MongoDB Setup

1. Sign up at [MongoDB Atlas](https://www.mongodb.com/atlas).
2. Create project â†’ Create cluster (M0 free tier).
3. Add DB user (username/password).
4. Network access â†’ Add IP `0.0.0.0/0`.
5. Get **Python connection string** â†’ replace password.
6. Create `notebook/mongoDB_demo.ipynb` â†’ load dataset â†’ push to MongoDB.
7. Verify in **Atlas Dashboard â†’ Collections**.

---

## ğŸ“ Logging, Exceptions & EDA

* Create `logger.py` & `exception.py`, test via `demo.py`.
* Add **EDA & Feature Engineering** notebooks.

---

## ğŸ“¥ Data Ingestion

* Define constants in `constants/__init__.py`.
* Implement MongoDB connection in `configuration/mongo_db_connections.py`.
* Build ingestion logic in `data_access/` + `entity/` classes.
* Run pipeline via `demo.py` after setting MongoDB URL:

```bash
# Bash
export MONGODB_URL="mongodb+srv://<username>:<password>@cluster0.mongodb.net/..."
echo $MONGODB_URL

# PowerShell
$env:MONGODB_URL="mongodb+srv://<username>:<password>@cluster0.mongodb.net/..."
echo $env:MONGODB_URL
```

> Add `artifact/` to `.gitignore`.

---

## ğŸ” Data Validation, Transformation & Training

* Define schema in `config/schema.yaml`.
* Implement:

  * **Data Validation**
  * **Data Transformation** (add `estimator.py`)
  * **Model Trainer** (extend `estimator.py`)

---

## â˜ï¸ AWS Setup

1. Create IAM user (`firstproj`) with **AdministratorAccess**.
2. Generate access keys â†’ save CSV.
3. Set env variables:

```bash
# Bash
export AWS_ACCESS_KEY_ID="..."
export AWS_SECRET_ACCESS_KEY="..."
echo $AWS_ACCESS_KEY_ID

# PowerShell
$env:AWS_ACCESS_KEY_ID="..."
$env:AWS_SECRET_ACCESS_KEY="..."
echo $env:AWS_ACCESS_KEY_ID
```

4. Create **S3 bucket** (`my-model-mlopsproj`).
5. Add AWS config code in `src/configuration/aws_connection.py` & `entity/s3_estimator.py`.

---

## ğŸ“Š Model Evaluation & Pusher

* Implement evaluation & model registry (S3).
* Build prediction pipeline via `app.py`.
* Add `static/` & `templates/` dirs for Flask app.

---

## ğŸ³ Docker & CI/CD

### Docker Setup

```bash
# Dockerfile + .dockerignore
docker build -t vehicleproj .
docker run -p 5080:5080 vehicleproj
```

### GitHub Actions

* Workflow file in `.github/workflows/aws.yaml`.
* Create IAM user (`usvisa-user`) â†’ generate access keys.
* Setup **ECR repo** (`vehicleproj`).
* Setup **EC2 Ubuntu server** (t2.medium).
* Install Docker on EC2.
* Configure **GitHub self-hosted runner** on EC2.
* Add GitHub secrets:

  * `AWS_ACCESS_KEY_ID`
  * `AWS_SECRET_ACCESS_KEY`
  * `AWS_DEFAULT_REGION`
  * `ECR_REPO`

Pipeline auto-triggers on `git push`.

---

## ğŸš€ Deployment

* Open EC2 â†’ Security Groups â†’ inbound rule:

  * TCP | Port: `5080` | Source: `0.0.0.0/0`.
* Launch app at:

```
http://<EC2_PUBLIC_IP>:5080
```

* Model training available at:

```
/training
```

---

## âœ… Summary

This project demonstrates:

* Modular ML pipeline design
* MongoDB Atlas integration
* AWS S3, ECR, EC2 setup
* CI/CD with GitHub Actions
* Dockerized deployment

---

âœ¨ With this pipeline, you can go from **raw data â†’ training â†’ deployment** fully automated with CI/CD.

